%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Title: nginx upstream
%Class: nginx
%Post:1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{前言}

upstream 是 nginx 中特殊的一个子系统. 本质上这是一个框架, 一个用于反向代理的框架.
http proxy 之类是在这个框架上实现的针对于 http 协议的反向代理.
还有一些其它的协议的模块如 fastcgi 之类.

而另一方面, upstream 这个框架本身还有一些子功能点 keepalive, load balancer.
同时作为 load balancer  也是一个框架的, 所以可以有很多不同的 balancer 算法如 ip_hash,
round_robin. 而 keepalive 又也是基于 balancer 实现的一个特殊的与 upstream 框架的关系比较紧的
特殊的模块.

另一方面在 upstream 还实现一个编译可选的功能: cache.

可见 upstream 实在是一个比较复杂的模块. 这些功能分成两个部份, 一部份是对于 upstream server
的管理也就是负载平衡的实现, 另一部分是对于一个反向代理的框架.

这里对于框架进行说明.

\section{使用}
由于upstream 是一个框架, 就像一是个类, 各个功能模块继承这个框架, 形成自己的类, 而当
用户进行调用的时候就是把这个功能类进行实例化.

比如: proxy 继承这 upstream 框架, 形成了自己的类, 用户使用 proxy_pass 对这个类进行实际化.

我们就通过 proxy_pass 对于这个 upstream 进行说明.

\section{实例化}

proxy_pass 命令的回调是  ngx_http_proxy_pass@ngx_http_proxy_modules.c .

一个 location 中的 handler 是在 content phase 的处理方法. 而 proxy_pass 命令把自己的处理方法
ngx_http_proxy_handler 挂载到当前的 location 上, 当一个请求处理到的 content 阶段的时候就是调用这个方法.

后面的代码就是对于参数的处理了.

我们来看看 ngx_http_proxy_handler 这个函数.

\starttyping
static ngx_int_t
ngx_http_proxy_handler(ngx_http_request_t *r)
{
    ngx_int_t                    rc;
    ngx_http_upstream_t         *u;
    ngx_http_proxy_ctx_t        *ctx;
    ngx_http_proxy_loc_conf_t   *plcf;
#if (NGX_HTTP_CACHE)
    ngx_http_proxy_main_conf_t  *pmcf;
#endif

    // 首先创建一个 upstream . 这是很重要, 也是我们 upstream 开始发挥作用的开始
    if (ngx_http_upstream_create(r) != NGX_OK) {
        return NGX_HTTP_INTERNAL_SERVER_ERROR;
    }

    // 设置 ctx, 没什么可说的了
    ctx = ngx_pcalloc(r->pool, sizeof(ngx_http_proxy_ctx_t));
    if (ctx == NULL) {
        return NGX_ERROR;
    }

    ngx_http_set_ctx(r, ctx, ngx_http_proxy_module);

    plcf = ngx_http_get_module_loc_conf(r, ngx_http_proxy_module);

    u = r->upstream; // 刚刚创建的

    if (plcf->proxy_lengths == NULL) {
        ctx->vars = plcf->vars;
        u->schema = plcf->vars.schema;
#if (NGX_HTTP_SSL)
        u->ssl = (plcf->upstream.ssl != NULL);
#endif

    } else {  // proxy_pass 的参数可以是 ngx script, 所以在这里对于可变的参数进行解析.
        if (ngx_http_proxy_eval(r, ctx, plcf) != NGX_OK) {
            return NGX_HTTP_INTERNAL_SERVER_ERROR;
        }
    }

    u->output.tag = (ngx_buf_tag_t) &ngx_http_proxy_module;

    u->conf = &plcf->upstream;

#if (NGX_HTTP_CACHE)
    pmcf = ngx_http_get_module_main_conf(r, ngx_http_proxy_module);

    u->caches = &pmcf->caches;
    u->create_key = ngx_http_proxy_create_key;
#endif

    u->create_request = ngx_http_proxy_create_request;
    u->reinit_request = ngx_http_proxy_reinit_request;
    u->process_header = ngx_http_proxy_process_status_line;
    u->abort_request = ngx_http_proxy_abort_request;
    u->finalize_request = ngx_http_proxy_finalize_request;
    r->state = 0;

    if (plcf->redirects) {
        u->rewrite_redirect = ngx_http_proxy_rewrite_redirect;
    }

    if (plcf->cookie_domains || plcf->cookie_paths) {
        u->rewrite_cookie = ngx_http_proxy_rewrite_cookie;
    }

    u->buffering = plcf->upstream.buffering;

    u->pipe = ngx_pcalloc(r->pool, sizeof(ngx_event_pipe_t));
    if (u->pipe == NULL) {
        return NGX_HTTP_INTERNAL_SERVER_ERROR;
    }

    u->pipe->input_filter = ngx_http_proxy_copy_filter;
    u->pipe->input_ctx = r;

    u->input_filter_init = ngx_http_proxy_input_filter_init;
    u->input_filter = ngx_http_proxy_non_buffered_copy_filter;
    u->input_filter_ctx = r;

    u->accel = 1;

    // goto upstream
    rc = ngx_http_read_client_request_body(r, ngx_http_upstream_init);

    if (rc >= NGX_HTTP_SPECIAL_RESPONSE) {
        return rc;
    }

    return NGX_DONE;
}
\stoptyping

create upstream 的处理也是很简单, 就是分配了内存并初始化了一些变量. 但是一个请求有可能有多个 upstream,
所以如果有必要会对于 upstream 进行 cleanup 操作.

在创建了 upstream 后就是设置了一系列的回调函数, upstream 是一个框架, 这个框架里一些细节的东西要依据不同的功能
使用不同的方法处理. 同时还设置一个 upstream 的配置信息如: buffering.

\starttable
\NC create_request \VL	生成发送到后端服务器的请求缓冲（缓冲链），在初始化upstream 时使用。\AR
\NC reinit_request	\VL在某台后端服务器出错的情况，nginx会尝试另一台后端服务器。 nginx选定新的服务器以后，会先调用此函数，以重新初始化 upstream模块的工作状态，然后再次进行upstream连接。\AR
\NC process_header	\VL处理后端服务器返回的信息头部。所谓头部是与upstream server 通信的协议规定的，比如HTTP协议的header部分，或者memcached 协议的响应状态部分。\AR
\NC abort_request	\VL在客户端放弃请求时被调用。不需要在函数中实现关闭后端服务 器连接的功能，系统会自动完成关闭连接的步骤，所以一般此函 数不会进行任何具体工作。\AR
\NC finalize_request\VL	正常完成与后端服务器的请求后调用该函数，与abort_request 相同，一般也不会进行任何具体工作。\AR
\NC input_filter	\VL处理后端服务器返回的响应正文。nginx默认的input_filter会 将收到的内容封装成为缓冲区链ngx_chain。该链由upstream的 out_bufs指针域定位，所以开发人员可以在模块以外通过该指针 得到后端服务器返回的正文数据。memcached模块实现了自己的 input_filter，在后面会具体分析这个模块。\AR
\NC input_filter_init	\VL初始化input filter的上下文。nginx默认的input_filter_init 直接返回。\AR
\stoptable

这些回调后面都会遇到进行详细的说明.

我们从这里就开启进入了 upstream 的逻辑里去了.

\subsection{ngx_http_upstream_init_request}
ngx_http_upstream_init并没有什么特别的工作, 直接调用了 ngx_http_upstream_init_request,
这个函数很长, 其实我是不太喜欢这么长的函数的.

这个函数开始注册两个函数用于检查与客户端的连接是否可用:
\starttyping
    if (!u->store && !r->post_action && !u->conf->ignore_client_abort) {
        r->read_event_handler = ngx_http_upstream_rd_check_broken_connection;
        r->write_event_handler = ngx_http_upstream_wr_check_broken_connection;
    }
\stoptyping

之后从请求中得到请求体:
\starttyping
    if (r->request_body) {
        u->request_bufs = r->request_body->bufs;
    }
\stoptyping

接下来就是调用 create_request 来创建请求. 之后就是使用对于 r->upstream_states 进行初始化.
再之后就挂载一些清理函数.

下面就是对于要连接的地址进行选择. 如果是使用 balancer 那么就是会进入到:
\starttyping

found:

    if (uscf == NULL) {
        ngx_log_error(NGX_LOG_ALERT, r->connection->log, 0,
                      "no upstream configuration");
        ngx_http_upstream_finalize_request(r, u,
                                           NGX_HTTP_INTERNAL_SERVER_ERROR);
        return;
    }

#if (NGX_HTTP_SSL)
    u->ssl_name = uscf->host;
#endif

    if (uscf->peer.init(r, uscf) != NGX_OK) { // 初始化 balancer
        ngx_http_upstream_finalize_request(r, u,
                                           NGX_HTTP_INTERNAL_SERVER_ERROR);
        return;
    }

    u->peer.start_time = ngx_current_msec;

    if (u->conf->next_upstream_tries  // 全局的重次参数
        && u->peer.tries > u->conf->next_upstream_tries)
    {
        u->peer.tries = u->conf->next_upstream_tries;
    }

    ngx_http_upstream_connect(r, u); // 连接上流服务器.


\stoptyping

\subsection{ngx_http_upstream_connect}
这个函数是 upstream 中很重要的函数, 这里已经开始了对于上游服务的连接.

\starttyping
static void
ngx_http_upstream_connect(ngx_http_request_t *r, ngx_http_upstream_t *u)
{
    ngx_int_t          rc;
    ngx_time_t        *tp;
    ngx_connection_t  *c;

    r->connection->log->action = "connecting to upstream";

    // connect 函数是一个可以重入的函数, 有可能已经连接过了, 但是失败了这个时候
    // 记录下前一次使用的时候
    if (u->state && u->state->response_sec) {
        tp = ngx_timeofday();
        u->state->response_sec = tp->sec - u->state->response_sec;
        u->state->response_msec = tp->msec - u->state->response_msec;
    }

    // 从 upstream_states 中得到下一个可以使用的空间
    u->state = ngx_array_push(r->upstream_states);
    if (u->state == NULL) {
        ngx_http_upstream_finalize_request(r, u,
                                           NGX_HTTP_INTERNAL_SERVER_ERROR);
        return;
    }

    ngx_memzero(u->state, sizeof(ngx_http_upstream_state_t));

    // 记录当前的时间
    tp = ngx_timeofday();
    u->state->response_sec = tp->sec;  //  用于记录处理整个响应时间
    u->state->response_msec = tp->msec;

    // 用于记录收到的响应头的时候, 这个时候没有记录, 实现的技巧, 从后面可以理解
    u->state->header_sec = (time_t) NGX_ERROR;

    // 连接到服务器生成一个可用的连接. 这个函数中会调用 balancer 中的 get 函数得到地址.
    // 然后去连接服务器.
    // 但是如果开启了 keepalive. keepalive 会返回 NGX_DONE 不连接到新的连接. 而是直接
    // 使用旧的连接, ngx_event_connect_peer 会直接退出.
    rc = ngx_event_connect_peer(&u->peer);

    ngx_log_debug1(NGX_LOG_DEBUG_HTTP, r->connection->log, 0,
                   "http upstream connect: %i", rc);

    // 可以是 balancer 返回, 也可以是 ngx_event_connect_peer 返回
    if (rc == NGX_ERROR) {
        ngx_http_upstream_finalize_request(r, u,
                                           NGX_HTTP_INTERNAL_SERVER_ERROR);
        return;
    }

    u->state->peer = u->peer.name;

    // balancer 可能会返回 NGX_BUSY 这个时候会返回 502
    if (rc == NGX_BUSY) {
        ngx_log_error(NGX_LOG_ERR, r->connection->log, 0, "no live upstreams");
        ngx_http_upstream_next(r, u, NGX_HTTP_UPSTREAM_FT_NOLIVE);
        return;
    }

    if (rc == NGX_DECLINED) {
        ngx_http_upstream_next(r, u, NGX_HTTP_UPSTREAM_FT_ERROR);
        return;
    }

    /* rc == NGX_OK || rc == NGX_AGAIN || rc == NGX_DONE */

    c = u->peer.connection;

    c->data = r;

    c->write->handler = ngx_http_upstream_handler;
    c->read->handler = ngx_http_upstream_handler;

    // 写事件: 发送请求
    u->write_event_handler = ngx_http_upstream_send_request_handler;
    // 读事件: 处理响应头
    u->read_event_handler = ngx_http_upstream_process_header;

    c->sendfile &= r->connection->sendfile;
    u->output.sendfile = c->sendfile;

    if (c->pool == NULL) {

        /* we need separate pool here to be able to cache SSL connections */

        c->pool = ngx_create_pool(128, r->connection->log);
        if (c->pool == NULL) {
            ngx_http_upstream_finalize_request(r, u,
                                               NGX_HTTP_INTERNAL_SERVER_ERROR);
            return;
        }
    }

    c->log = r->connection->log;
    c->pool->log = c->log;
    c->read->log = c->log;
    c->write->log = c->log;

    /* init or reinit the ngx_output_chain() and ngx_chain_writer() contexts */

    u->writer.out = NULL;
    u->writer.last = &u->writer.out;
    u->writer.connection = c;
    u->writer.limit = 0;

    if (u->request_sent) {
        if (ngx_http_upstream_reinit(r, u) != NGX_OK) {
            ngx_http_upstream_finalize_request(r, u,
                                               NGX_HTTP_INTERNAL_SERVER_ERROR);
            return;
        }
    }

    if (r->request_body
        && r->request_body->buf
        && r->request_body->temp_file
        && r == r->main)
    {
        /*
         * the r->request_body->buf can be reused for one request only,
         * the subrequests should allocate their own temporary bufs
         */

        u->output.free = ngx_alloc_chain_link(r->pool);
        if (u->output.free == NULL) {
            ngx_http_upstream_finalize_request(r, u,
                                               NGX_HTTP_INTERNAL_SERVER_ERROR);
            return;
        }

        u->output.free->buf = r->request_body->buf;
        u->output.free->next = NULL;
        u->output.allocated = 1;

        r->request_body->buf->pos = r->request_body->buf->start;
        r->request_body->buf->last = r->request_body->buf->start;
        r->request_body->buf->tag = u->output.tag;
    }

    u->request_sent = 0;

    if (rc == NGX_AGAIN) { // TODO 这里没有明白
        ngx_add_timer(c->write, u->conf->connect_timeout);
        return;
    }

#if (NGX_HTTP_SSL)

    if (u->ssl && c->ssl == NULL) {
        ngx_http_upstream_ssl_init_connection(r, u, c);
        return;
    }

#endif

    // 对网络进行测试, 对于定时进行处理
    ngx_http_upstream_send_request(r, u);
}
\stoptyping


发送请求:
\starttyping
static void
ngx_http_upstream_send_request(ngx_http_request_t *r, ngx_http_upstream_t *u)
{
    ngx_int_t          rc;
    ngx_connection_t  *c;

    c = u->peer.connection;

    ngx_log_debug0(NGX_LOG_DEBUG_HTTP, c->log, 0,
                   "http upstream send request");

    // 没有发送过请求, 就对于网络进行一下测试
    if (!u->request_sent && ngx_http_upstream_test_connect(c) != NGX_OK) {
        ngx_http_upstream_next(r, u, NGX_HTTP_UPSTREAM_FT_ERROR);
        return;
    }

    c->log->action = "sending request to upstream";

    // 发送数据
    rc = ngx_output_chain(&u->output, u->request_sent ? NULL : u->request_bufs);

    u->request_sent = 1;

    if (rc == NGX_ERROR) {
        ngx_http_upstream_next(r, u, NGX_HTTP_UPSTREAM_FT_ERROR);
        return;
    }

    if (c->write->timer_set) {
        ngx_del_timer(c->write);
    }

    // 数据没有发送完成, 增加事件, 继续发送
    if (rc == NGX_AGAIN) {
        ngx_add_timer(c->write, u->conf->send_timeout);

        if (ngx_handle_write_event(c->write, u->conf->send_lowat) != NGX_OK) {
            ngx_http_upstream_finalize_request(r, u,
                                               NGX_HTTP_INTERNAL_SERVER_ERROR);
            return;
        }

        return;
    }

    /* rc == NGX_OK */

    if (c->tcp_nopush == NGX_TCP_NOPUSH_SET) {
        if (ngx_tcp_push(c->fd) == NGX_ERROR) {
            ngx_log_error(NGX_LOG_CRIT, c->log, ngx_socket_errno,
                          ngx_tcp_push_n " failed");
            ngx_http_upstream_finalize_request(r, u,
                                               NGX_HTTP_INTERNAL_SERVER_ERROR);
            return;
        }

        c->tcp_nopush = NGX_TCP_NOPUSH_UNSET;
    }

    // 不会再写了
    u->write_event_handler = ngx_http_upstream_dummy_handler;

    if (ngx_handle_write_event(c->write, 0) != NGX_OK) {
        ngx_http_upstream_finalize_request(r, u,
                                           NGX_HTTP_INTERNAL_SERVER_ERROR);
        return;
    }

    // 增加读定时器
    ngx_add_timer(c->read, u->conf->read_timeout);

    if (c->read->ready) {
        //如果发送成功处理响应
        ngx_http_upstream_process_header(r, u);
        return;
    }
}

\stoptyping


\section{ngx_http_upstream_process_header}
这个函数完成接收数据与调用 process_header 的功能.
这里说明一个大体的过程, process_header 在 http proxy 里的回调一开始是 ngx_http_proxy_process_status_line
而这个函数处理完成的时候, 并不是直接返回成功而是把这个回调再次修改成 ngx_http_proxy_process_header.
这样的结果就是 upstream 同一个回调支持起了对于响应行与响应头的两个阶段的处理.

\starttyping

    tp = ngx_timeofday();
    u->state->header_sec = tp->sec - u->state->response_sec; // 记录接收到响应头用了多少时间
    u->state->header_msec = tp->msec - u->state->response_msec;

    // 注意这里, 要对于状态态进行分析, 也就是说这里默认的情况下认为响应的是 http
    // 因为这里的结果要返回给用户, 所以就算请求出动不是 http, 响应应该是http, 比如 fastcgi
    // 如果响应不是 http 比如 memcached, 就应该在模块里设置 status_n
    if (u->headers_in.status_n >= NGX_HTTP_SPECIAL_RESPONSE) {

        if (ngx_http_upstream_test_next(r, u) == NGX_OK) {
            return;
        }

        if (ngx_http_upstream_intercept_errors(r, u) == NGX_OK) {
            return;
        }
    }

    // 对于响应进行检查, 并copy
    if (ngx_http_upstream_process_headers(r, u) != NGX_OK) {
        return;
    }

    // 如果不使用缓存, 立即返回给用户
    if (!r->subrequest_in_memory) { // 没有子请求, 直接发送响应给客户端
        ngx_http_upstream_send_response(r, u);
        return;
    }

    /* subrequest content in memory */

    if (u->input_filter == NULL) {
        u->input_filter_init = ngx_http_upstream_non_buffered_filter_init;
        u->input_filter = ngx_http_upstream_non_buffered_filter;
        u->input_filter_ctx = r;
    }

    if (u->input_filter_init(u->input_filter_ctx) == NGX_ERROR) {
        ngx_http_upstream_finalize_request(r, u, NGX_ERROR);
        return;
    }

    ........

    u->read_event_handler = ngx_http_upstream_process_body_in_memory;

    ngx_http_upstream_process_body_in_memory(r, u);
\stoptyping
这里就处理到了给用户发送数据的阶段了, 这里有两种情况 1. buffering 2. no buffering
前者是接收一些数据发送一些, 后者是缓存一定的数据后再发送.

\section{buffering}
这种情况 ngx 会使用一个缓存, 不同于 no buffering 的处理过程. 相对的也会比较复杂一些.
好处是可以平衡之间的速度.

这个过程中使用了 event_pipe, 用于 upstream 与 client 之间.
下面是其核心的数据结构:
\starttyping
struct ngx_event_pipe_s {//nginx处理buffering机制的结构
    ngx_connection_t  *upstream;//表示nginx和后端的两条连接
    ngx_connection_t  *downstream;//这个表示客户端的连接

    ngx_chain_t       *free_raw_bufs;//保存了从upstream读取的数据(没有经过任何处理的)，以及缓存的buf.
    ngx_chain_t       *in;
    //每次读取数据后，调用input_filter对协议格式进行解析，解析完后的数据部分放到in里面形成一个链表。
    /*关于p->in和shadow，多说一下，in指向一堆chain链表，每个链表指向一块实实在在的fcgi DATA数据，
      多个这样的html代码块共享一块大的裸FCGI数据块；
      属于某个大的裸FCGI数据块的最后一个数据节点的last_shadow成员为1，
      表示我是这个大FCGI数据块的最后一个，并且我的shadow指针指向这个裸FCGI数据块的buf指针
      释放这些大数据块的时候，可以参考ngx_event_pipe_drain_chains进行释放。
     */

    ngx_chain_t      **last_in;//上面的in结构的最后一个节点的next指针的地址，p->last_in = &cl->next;，
                    这样就可以将新分析到的FCGI数据链接到后面了。

    //out 指放入文件中的buffer，其总是在in所指向的buf链表的前面，发送前先发送这个

    ngx_chain_t       *out;//buf到tempfile的数据会放到out里面。在ngx_event_pipe_write_chain_to_temp_file函数里面设置的。

    ngx_chain_t      **last_out;


    ngx_chain_t       *free;//这里就是那些空闲的内存节点，从busy移动过来的。注意是节点，不是buf
    ngx_chain_t       *busy;
    //代表经过了output_filter过的，从out移动过来的缓存，其里面可能有已经发送完成了，
    //因为ngx_http_write_filter会更新这写buf的

    /*
     * the input filter i.e. that moves HTTP/1.1 chunks
     * from the raw bufs to an incoming chain
     *///FCGI为ngx_http_fastcgi_input_filter，其他为ngx_event_pipe_copy_input_filter 。用来解析特定格式数据
    ngx_event_pipe_input_filter_pt    input_filter;//这个用来解析对应协议的数据。比如解析FCGI协议的数据。
    void                             *input_ctx;



    ngx_event_pipe_output_filter_pt   output_filter;//ngx_http_output_filter输出filter

    void                             *output_ctx;



    unsigned           read:1;//标记是否读了数据。
    unsigned           cacheable:1;
    unsigned           single_buf:1;//如果使用了NGX_USE_AIO_EVENT异步IO标志，则设置为1
    unsigned           free_bufs:1;
    unsigned           upstream_done:1;
    unsigned           upstream_error:1;
    unsigned           upstream_eof:1;
    unsigned           upstream_blocked:1;//ngx_event_pipe用来标记是否读取了upstream的数据来决定是不是要write
    unsigned           downstream_done:1;
    unsigned           downstream_error:1;
    unsigned           cyclic_temp_file:1;

    ngx_int_t          allocated;//表示已经分配了的bufs的个数，每次会++
    //fastcgi_buffers等指令设置的nginx用来缓存body的内存块数目以及大小。ngx_conf_set_bufs_slot函数会解析这样的配置。
    //对应xxx_buffers,也就是读取后端的数据时的bufer大小以及个数
    ngx_bufs_t         bufs;
    ngx_buf_tag_t      tag;

    ssize_t            busy_size;//fastcgi_busy_buffers_size 指令或者其他upstream设置的大小，作用为最大的busy状态的内存总容量。

    //文档介绍 : mits the total size of buffers that can be busy sending a response to the client while the response is not yet fully read.



    off_t              read_length;//从upstream读取的数据长度

    off_t              max_temp_file_size;
    ssize_t            temp_file_write_size;
    ngx_msec_t         read_timeout;
    ngx_msec_t         send_timeout;
    ssize_t            send_lowat;

    ngx_pool_t        *pool;
    ngx_log_t         *log;



    //指读取upstream的时候多读的，或者说预读的body部分数据。p->preread_bufs->buf = &u->buffer;
    ngx_chain_t       *preread_bufs;
    size_t             preread_size;
    ngx_buf_t         *buf_to_file;

    ngx_temp_file_t   *temp_file;
    /* STUB */ int     num;

};
\stoptyping


下面是 ngx_http_upstream_send_response 关于 buffering 开启了的情况下, 对于 pip 进行初始化的代码:
\starttyping
    p = u->pipe;

    p->output_filter = (ngx_event_pipe_output_filter_pt) ngx_http_output_filter;
    p->output_ctx = r;
    p->tag = u->output.tag;
    p->bufs = u->conf->bufs;// 配置的 buf 的大小与个数
    p->busy_size = u->conf->busy_buffers_size;
    p->upstream = u->peer.connection; // 与后端的连接
    p->downstream = c; // 与 client 的连接
    p->pool = r->pool;
    p->log = c->log;
    p->limit_rate = u->conf->limit_rate;
    p->start_sec = ngx_time();

    p->cacheable = u->cacheable || u->store;

    p->temp_file = ngx_pcalloc(r->pool, sizeof(ngx_temp_file_t));
    if (p->temp_file == NULL) {
        ngx_http_upstream_finalize_request(r, u, NGX_ERROR);
        return;
    }

    p->temp_file->file.fd = NGX_INVALID_FILE;
    p->temp_file->file.log = c->log;
    p->temp_file->path = u->conf->temp_path;
    p->temp_file->pool = r->pool;

    if (p->cacheable) {
        p->temp_file->persistent = 1;

    } else {
        p->temp_file->log_level = NGX_LOG_WARN;
        p->temp_file->warn = "an upstream response is buffered "
                             "to a temporary file";
    }

    p->max_temp_file_size = u->conf->max_temp_file_size;
    p->temp_file_write_size = u->conf->temp_file_write_size;

    //指读取upstream的时候多读的，或者说预读的body部分数据。p->preread_bufs->buf = &u->buffer;
    p->preread_bufs = ngx_alloc_chain_link(r->pool);
    if (p->preread_bufs == NULL) {
        ngx_http_upstream_finalize_request(r, u, NGX_ERROR);
        return;
    }

    p->preread_bufs->buf = &u->buffer;
    p->preread_bufs->next = NULL;
    u->buffer.recycled = 1;

    p->preread_size = u->buffer.last - u->buffer.pos;

    if (u->cacheable) {

        p->buf_to_file = ngx_calloc_buf(r->pool);
        if (p->buf_to_file == NULL) {
            ngx_http_upstream_finalize_request(r, u, NGX_ERROR);
            return;
        }

        p->buf_to_file->start = u->buffer.start;
        p->buf_to_file->pos = u->buffer.start;
        p->buf_to_file->last = u->buffer.pos;
        p->buf_to_file->temporary = 1;
    }

    if (ngx_event_flags & NGX_USE_AIO_EVENT) {
        /* the posted aio operation may corrupt a shadow buffer */
        p->single_buf = 1;
    }

    /* TODO: p->free_bufs = 0 if use ngx_create_chain_of_bufs() */
    p->free_bufs = 1;

    /*
     * event_pipe would do u->buffer.last += p->preread_size
     * as though these bytes were read
     */
    u->buffer.last = u->buffer.pos;

    if (u->conf->cyclic_temp_file) {

        /*
         * we need to disable the use of sendfile() if we use cyclic temp file
         * because the writing a new data may interfere with sendfile()
         * that uses the same kernel file pages (at least on FreeBSD)
         */

        p->cyclic_temp_file = 1;
        c->sendfile = 0;

    } else {
        p->cyclic_temp_file = 0;
    }

    p->read_timeout = u->conf->read_timeout;
    p->send_timeout = clcf->send_timeout;
    p->send_lowat = clcf->send_lowat;

    p->length = -1;

    if (u->input_filter_init
        && u->input_filter_init(p->input_ctx) != NGX_OK)
    {
        ngx_http_upstream_finalize_request(r, u, NGX_ERROR);
        return;
    }

    u->read_event_handler = ngx_http_upstream_process_upstream;
    r->write_event_handler = ngx_http_upstream_process_downstream;

    ngx_http_upstream_process_upstream(r, u);
\stoptyping
完成回调的情况下 调用 ngx_http_upstream_process_upstream 进行处理.
这个函数也是回调函数, 在 upstream 可读的情况下 以及 下流可写的情况下会调用这个函数.
\starttyping
static void
ngx_http_upstream_process_upstream(ngx_http_request_t *r,
    ngx_http_upstream_t *u)
{
   ......

    if (rev->timedout) {
      ...... // 处理超时

    } else {
        ......

        if (ngx_event_pipe(p, 0) == NGX_ABORT) { // 调用 pip 完成处理
            ngx_http_upstream_finalize_request(r, u, NGX_ERROR);
            return;
        }
    }

    ngx_http_upstream_process_request(r); // //处理了一下是否需要吧数据写到磁盘上
}
\stoptyping

到这里就进入了 ngx_event_pipe. 关于pipe 的实现是一个单独的文件中实现的:
src/event/ngx_event_pipe.c . 这里实现了关于pipe 的逻辑.

\section{ngx_event_pipe}
pipe 的目的就把后端的数据发送到前端, 这个过程要不断地从后端读取数据, 再向前端写入.

所以这个函数就是一断循环. 如果第二个参数为真, 这个循环第一次开始就会先向 downstream 写数据,
否则就会先读取数据, 如果成功再向后端写数据.
\starttyping
    for ( ;; ) {
        if (do_write) {
            p->log->action = "sending to client";

            rc = ngx_event_pipe_write_to_downstream(p);

            if (rc == NGX_ABORT) {
                return NGX_ABORT;
            }

            if (rc == NGX_BUSY) {
                return NGX_OK;
            }
        }

        p->read = 0;
        p->upstream_blocked = 0;

        p->log->action = "reading upstream";

        if (ngx_event_pipe_read_upstream(p) == NGX_ABORT) {
            return NGX_ABORT;
        }

        if (!p->read && !p->upstream_blocked) {
            break;
        }

        do_write = 1;
    }
\stoptyping

这里两个实现分别是 ngx_event_pipe_read_upstream, ngx_event_pipe_write_to_downstream.
这两个函数是实现这个过程的主要逻辑.
如下是调用过程:
\startitemize
\item ngx_single_process_cycle循环调用ngx_process_events_and_timers，后者调用ngx_epoll_process_events处理读写事件；
\item c->read->handler = ngx_http_upstream_handler()；SOCK连接最基础的读写回调handler，
\item u->read_event_handler = ngx_http_upstream_process_upstream()；
\item ngx_event_pipe()；
\item ngx_event_pipe_read_upstream()/ngx_event_pipe_write_to_downstream() 进入主要读取或写入数据 。
\stopitemize

后面对于 read upstream 与 write to downstream 的说明, 后面再写.




















